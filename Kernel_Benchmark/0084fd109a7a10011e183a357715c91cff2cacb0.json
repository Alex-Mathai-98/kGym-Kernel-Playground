{
    "version": 1,
    "title": "WARNING in alloc_pages_vma",
    "display-title": "WARNING in alloc_pages_vma",
    "id": "0084fd109a7a10011e183a357715c91cff2cacb0",
    "status": "fixed",
    "fix-commits": [
        {
            "title": "Revert \"mm, thp: consolidate THP gfp handling into alloc_hugepage_direct_gfpmask\"",
            "link": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=356ff8a9a78fb35d6482584d260c3754dcbdf669",
            "hash": "356ff8a9a78fb35d6482584d260c3754dcbdf669",
            "repo": "git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git",
            "branch": "master"
        }
    ],
    "crashes": [
        {
            "title": "",
            "syz-reproducer": "/text?tag=ReproSyz&x=10f7ec15400000",
            "c-reproducer": "/text?tag=ReproC&x=14b6796d400000",
            "kernel-config": "/text?tag=KernelConfig&x=b9cc5a440391cbfd",
            "kernel-source-git": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/log/?id=cf76c364a1e1e5224af80edf70a1e3023e1fcf8c",
            "kernel-source-commit": "cf76c364a1e1e5224af80edf70a1e3023e1fcf8c",
            "syzkaller-git": "https://github.com/google/syzkaller/commits/3ab38479ab60b50d5b27332bd001c78382877ecd",
            "syzkaller-commit": "3ab38479ab60b50d5b27332bd001c78382877ecd",
            "compiler-description": "gcc (GCC) 8.0.1 20180413 (experimental)",
            "architecture": "amd64",
            "crash-report-link": "/text?tag=CrashReport&x=157e88b3400000"
        }
    ],
    "subsystems": [
        "mm"
    ],
    "parent_of_fix_commit": "5f179793f0a73965681db6a3203fa1baabd9b3c3",
    "patch": "diff --git a/include/linux/gfp.h b/include/linux/gfp.h\nindex 76f8db0b0e71..0705164f928c 100644\n--- a/include/linux/gfp.h\n+++ b/include/linux/gfp.h\n@@ -510,18 +510,22 @@ alloc_pages(gfp_t gfp_mask, unsigned int order)\n }\n extern struct page *alloc_pages_vma(gfp_t gfp_mask, int order,\n \t\t\tstruct vm_area_struct *vma, unsigned long addr,\n-\t\t\tint node);\n+\t\t\tint node, bool hugepage);\n+#define alloc_hugepage_vma(gfp_mask, vma, addr, order) \\\n+\talloc_pages_vma(gfp_mask, order, vma, addr, numa_node_id(), true)\n #else\n #define alloc_pages(gfp_mask, order) \\\n \t\talloc_pages_node(numa_node_id(), gfp_mask, order)\n-#define alloc_pages_vma(gfp_mask, order, vma, addr, node)\\\n+#define alloc_pages_vma(gfp_mask, order, vma, addr, node, false)\\\n+\talloc_pages(gfp_mask, order)\n+#define alloc_hugepage_vma(gfp_mask, vma, addr, order) \\\n \talloc_pages(gfp_mask, order)\n #endif\n #define alloc_page(gfp_mask) alloc_pages(gfp_mask, 0)\n #define alloc_page_vma(gfp_mask, vma, addr)\t\t\t\\\n-\talloc_pages_vma(gfp_mask, 0, vma, addr, numa_node_id())\n+\talloc_pages_vma(gfp_mask, 0, vma, addr, numa_node_id(), false)\n #define alloc_page_vma_node(gfp_mask, vma, addr, node)\t\t\\\n-\talloc_pages_vma(gfp_mask, 0, vma, addr, node)\n+\talloc_pages_vma(gfp_mask, 0, vma, addr, node, false)\n \n extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);\n extern unsigned long get_zeroed_page(gfp_t gfp_mask);\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex f2d19e4fe854..5da55b38b1b7 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -629,30 +629,30 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,\n  *\t    available\n  * never: never stall for any thp allocation\n  */\n-static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma, unsigned long addr)\n+static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma)\n {\n \tconst bool vma_madvised = !!(vma->vm_flags & VM_HUGEPAGE);\n-\tconst gfp_t gfp_mask = GFP_TRANSHUGE_LIGHT | __GFP_THISNODE;\n \n \t/* Always do synchronous compaction */\n \tif (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG, &transparent_hugepage_flags))\n-\t\treturn GFP_TRANSHUGE | __GFP_THISNODE |\n-\t\t       (vma_madvised ? 0 : __GFP_NORETRY);\n+\t\treturn GFP_TRANSHUGE | (vma_madvised ? 0 : __GFP_NORETRY);\n \n \t/* Kick kcompactd and fail quickly */\n \tif (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_FLAG, &transparent_hugepage_flags))\n-\t\treturn gfp_mask | __GFP_KSWAPD_RECLAIM;\n+\t\treturn GFP_TRANSHUGE_LIGHT | __GFP_KSWAPD_RECLAIM;\n \n \t/* Synchronous compaction if madvised, otherwise kick kcompactd */\n \tif (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG, &transparent_hugepage_flags))\n-\t\treturn gfp_mask | (vma_madvised ? __GFP_DIRECT_RECLAIM :\n-\t\t\t\t\t\t  __GFP_KSWAPD_RECLAIM);\n+\t\treturn GFP_TRANSHUGE_LIGHT |\n+\t\t\t(vma_madvised ? __GFP_DIRECT_RECLAIM :\n+\t\t\t\t\t__GFP_KSWAPD_RECLAIM);\n \n \t/* Only do synchronous compaction if madvised */\n \tif (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG, &transparent_hugepage_flags))\n-\t\treturn gfp_mask | (vma_madvised ? __GFP_DIRECT_RECLAIM : 0);\n+\t\treturn GFP_TRANSHUGE_LIGHT |\n+\t\t       (vma_madvised ? __GFP_DIRECT_RECLAIM : 0);\n \n-\treturn gfp_mask;\n+\treturn GFP_TRANSHUGE_LIGHT;\n }\n \n /* Caller must hold page table lock. */\n@@ -724,8 +724,8 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)\n \t\t\tpte_free(vma->vm_mm, pgtable);\n \t\treturn ret;\n \t}\n-\tgfp = alloc_hugepage_direct_gfpmask(vma, haddr);\n-\tpage = alloc_pages_vma(gfp, HPAGE_PMD_ORDER, vma, haddr, numa_node_id());\n+\tgfp = alloc_hugepage_direct_gfpmask(vma);\n+\tpage = alloc_hugepage_vma(gfp, vma, haddr, HPAGE_PMD_ORDER);\n \tif (unlikely(!page)) {\n \t\tcount_vm_event(THP_FAULT_FALLBACK);\n \t\treturn VM_FAULT_FALLBACK;\n@@ -1295,9 +1295,8 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)\n alloc:\n \tif (transparent_hugepage_enabled(vma) &&\n \t    !transparent_hugepage_debug_cow()) {\n-\t\thuge_gfp = alloc_hugepage_direct_gfpmask(vma, haddr);\n-\t\tnew_page = alloc_pages_vma(huge_gfp, HPAGE_PMD_ORDER, vma,\n-\t\t\t\thaddr, numa_node_id());\n+\t\thuge_gfp = alloc_hugepage_direct_gfpmask(vma);\n+\t\tnew_page = alloc_hugepage_vma(huge_gfp, vma, haddr, HPAGE_PMD_ORDER);\n \t} else\n \t\tnew_page = NULL;\n \ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 69e278b469ef..d4496d9d34f5 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -1116,8 +1116,8 @@ static struct page *new_page(struct page *page, unsigned long start)\n \t} else if (PageTransHuge(page)) {\n \t\tstruct page *thp;\n \n-\t\tthp = alloc_pages_vma(GFP_TRANSHUGE, HPAGE_PMD_ORDER, vma,\n-\t\t\t\taddress, numa_node_id());\n+\t\tthp = alloc_hugepage_vma(GFP_TRANSHUGE, vma, address,\n+\t\t\t\t\t HPAGE_PMD_ORDER);\n \t\tif (!thp)\n \t\t\treturn NULL;\n \t\tprep_transhuge_page(thp);\n@@ -2011,6 +2011,7 @@ static struct page *alloc_page_interleave(gfp_t gfp, unsigned order,\n  * \t@vma:  Pointer to VMA or NULL if not available.\n  *\t@addr: Virtual Address of the allocation. Must be inside the VMA.\n  *\t@node: Which node to prefer for allocation (modulo policy).\n+ *\t@hugepage: for hugepages try only the preferred node if possible\n  *\n  * \tThis function allocates a page from the kernel page pool and applies\n  *\ta NUMA policy associated with the VMA or the current process.\n@@ -2021,7 +2022,7 @@ static struct page *alloc_page_interleave(gfp_t gfp, unsigned order,\n  */\n struct page *\n alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,\n-\t\tunsigned long addr, int node)\n+\t\tunsigned long addr, int node, bool hugepage)\n {\n \tstruct mempolicy *pol;\n \tstruct page *page;\n@@ -2039,6 +2040,31 @@ alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,\n \t\tgoto out;\n \t}\n \n+\tif (unlikely(IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) && hugepage)) {\n+\t\tint hpage_node = node;\n+\n+\t\t/*\n+\t\t * For hugepage allocation and non-interleave policy which\n+\t\t * allows the current node (or other explicitly preferred\n+\t\t * node) we only try to allocate from the current/preferred\n+\t\t * node and don't fall back to other nodes, as the cost of\n+\t\t * remote accesses would likely offset THP benefits.\n+\t\t *\n+\t\t * If the policy is interleave, or does not allow the current\n+\t\t * node in its nodemask, we allocate the standard way.\n+\t\t */\n+\t\tif (pol->mode == MPOL_PREFERRED && !(pol->flags & MPOL_F_LOCAL))\n+\t\t\thpage_node = pol->v.preferred_node;\n+\n+\t\tnmask = policy_nodemask(gfp, pol);\n+\t\tif (!nmask || node_isset(hpage_node, *nmask)) {\n+\t\t\tmpol_cond_put(pol);\n+\t\t\tpage = __alloc_pages_node(hpage_node,\n+\t\t\t\t\t\tgfp | __GFP_THISNODE, order);\n+\t\t\tgoto out;\n+\t\t}\n+\t}\n+\n \tnmask = policy_nodemask(gfp, pol);\n \tpreferred_nid = policy_node(gfp, pol, node);\n \tpage = __alloc_pages_nodemask(gfp, order, preferred_nid, nmask);\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex cddc72ac44d8..921f80488bb3 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -1439,7 +1439,7 @@ static struct page *shmem_alloc_hugepage(gfp_t gfp,\n \n \tshmem_pseudo_vma_init(&pvma, info, hindex);\n \tpage = alloc_pages_vma(gfp | __GFP_COMP | __GFP_NORETRY | __GFP_NOWARN,\n-\t\t\tHPAGE_PMD_ORDER, &pvma, 0, numa_node_id());\n+\t\t\tHPAGE_PMD_ORDER, &pvma, 0, numa_node_id(), true);\n \tshmem_pseudo_vma_destroy(&pvma);\n \tif (page)\n \t\tprep_transhuge_page(page);\n",
    "patch_modified_files": [
        "include/linux/gfp.h",
        "mm/huge_memory.c",
        "mm/mempolicy.c",
        "mm/shmem.c"
    ]
}